{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STT_TF_rnn_MultiContext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juglar-diaz/STTD/blob/master/STT_TF_rnn_MultiContext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02j_-p6TZ2r",
        "colab_type": "text"
      },
      "source": [
        "#Intro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3QjBnH6lktK",
        "colab_type": "code",
        "outputId": "b6431ee3-039c-4a67-fbd2-189503753744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xi19T6pJfwg",
        "colab_type": "code",
        "outputId": "458efe39-4512-4ff2-d872-96bb2d4f30be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGPfqOgRnsBQ",
        "colab_type": "code",
        "outputId": "743cdddf-7f7a-4d4d-d5dd-73139edae64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cu80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4n58fkig5KL",
        "colab_type": "code",
        "outputId": "df222cb4-9780-4c49-b540-bce349b97a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "sep = os.sep\n",
        "import os.path\n",
        "\n",
        "import pandas as pd\n",
        "import bisect\n",
        "import time\n",
        "import scipy.stats as stats\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqvwguvPKbzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = pd.Timestamp(year=2017, month=12, day=2, hour=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjOUwBA7KhRx",
        "colab_type": "code",
        "outputId": "9867c06b-d5e1-4266-a4cd-452ce9d07f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "t.weekday()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCXJs-EYKsGZ",
        "colab_type": "code",
        "outputId": "88606a5f-980d-4138-9901-136838f15d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "t.weekofyear"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f20pFCuuJ61h",
        "colab_type": "code",
        "outputId": "2ad542a2-8b0e-431e-f8e0-9b9f9e8d33d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if not tf.test.is_gpu_available():\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbVhhVRJ0F0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/STTD/Retrieval_TF/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY9cFt5HNOPy",
        "colab_type": "code",
        "outputId": "548772d8-5f2a-4724-f1f2-bea46f052cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oyyyhvZUsn2",
        "colab_type": "text"
      },
      "source": [
        "##Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CuzsQKOVIHX",
        "colab_type": "text"
      },
      "source": [
        "###Discretizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxTw1I4cUsB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discretize:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit_transform(self):\n",
        "        pass\n",
        "    def transform(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdDjfD35BdLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Round(Discretize):\n",
        "    def __init__(self, div= 0.001):\n",
        "        self.div= div\n",
        "    def discretize(self, latitudes, longitudes):\n",
        "        lats = [round(lat - lat%self.div + self.div/2, 6) for lat in latitudes]\n",
        "        longs = [round(lon - lon%self.div + self.div/2, 6) for lon in longitudes]\n",
        "        discretizations = list(zip(lats, longs))\n",
        "        return discretizations\n",
        "    \n",
        "    def fit_transform(self, latitudes, longitudes):\n",
        "        discretizations = self.discretize(latitudes, longitudes)\n",
        "        lats = list(latitudes)\n",
        "        longs = list(longitudes)\n",
        "        self.mapping = {disc:[] for disc in discretizations}\n",
        "        for i in range(len(discretizations)):\n",
        "            self.mapping[discretizations[i]].append((lats[i], longs[i]))\n",
        "        return discretizations\n",
        "\n",
        "    def transform(self, latitudes, longitudes):\n",
        "        return self.discretize(latitudes, longitudes)\n",
        "    \n",
        "    def measure(self, disc1, disc2):\n",
        "        dist_lat = disc1[0]-disc2[0]\n",
        "        dist_long = disc1[0]-disc2[0]\n",
        "        return math.sqrt(dist_lat*dist_lat+dist_long*dist_long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeLf95ghBYto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeDiscretizer(Discretize):\n",
        "    def discretize(self, created_at):\n",
        "        pass\n",
        "    \n",
        "    def fit_transform(self, created_at):\n",
        "        discretizations = self.discretize(created_at)\n",
        "        times = list(created_at)\n",
        "        self.mapping = {disc:[] for disc in discretizations}\n",
        "        for i in range(len(discretizations)):\n",
        "            self.mapping[discretizations[i]].append(times[i])\n",
        "        return discretizations   \n",
        "    \n",
        "    def transform(self, created_at):\n",
        "        return self.discretize(created_at)\n",
        "\n",
        "class Hour(TimeDiscretizer):\n",
        "    def discretize(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        return list(indi.hour)\n",
        "    \n",
        "class Day(TimeDiscretizer):\n",
        "    def discretize(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        return list(indi.weekday)\n",
        "\n",
        "class Week(TimeDiscretizer):\n",
        "    def discretize(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        return list(indi.hour%4)\n",
        "\n",
        "class Month(TimeDiscretizer):\n",
        "    def discretize(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        return list(indi.month)\n",
        "           \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQgX5cP9sBPv",
        "colab_type": "text"
      },
      "source": [
        "###Build vocab, index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEl_T7aLsJQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = \"\"\n",
        "\n",
        "def buildIndexData(list_elements):\n",
        "    idx2data = {index:element  for index,element in enumerate(sorted(set(list_elements)))}\n",
        "    data2idx = {element:index for index,element in idx2data.items()}\n",
        "    return data2idx, idx2data\n",
        "        \n",
        "class Indexer():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def build_vocab(self, l, vocab_size = 0, vocab_mincount = 0):\n",
        "        counter_l = Counter(l)\n",
        "        if (vocab_size > 0):\n",
        "            pairs = counter_l.most_common(vocab_size)\n",
        "        else:\n",
        "            pairs = list(counter_l.items())\n",
        "        return set([token for token, count in pairs if count >= vocab_mincount])\n",
        "\n",
        "    def fit_transform(self,\n",
        "            df,\n",
        "            dates_vocab_size = 0, dates_vocab_mincount = 0,\n",
        "            places_vocab_size = 0, places_vocab_mincount = 0,\n",
        "            words_vocab_size = 0, words_vocab_mincount = 0): \n",
        "\n",
        "        self.time_discretizer_hour = Hour()\n",
        "        self.time_discretizer_day = Day()\n",
        "        self.time_discretizer_week = Week()\n",
        "        self.time_discretizer_month = Month()\n",
        "\n",
        "        self.coor_discretizer_1 = Round(0.001)\n",
        "        self.coor_discretizer_2 = Round(0.002)\n",
        "        self.coor_discretizer_4 = Round(0.004)\n",
        "        self.coor_discretizer_8 = Round(0.008)\n",
        "\n",
        "       \n",
        "        #file_csv has columns created_at, latitude, longitude, text\n",
        "        dates = self.time_discretizer_hour.fit_transform(df['created_at'])\n",
        "        days = self.time_discretizer_day.fit_transform(df['created_at'])\n",
        "        weeks = self.time_discretizer_week.fit_transform(df['created_at'])\n",
        "        months = self.time_discretizer_month.fit_transform(df['created_at'])\n",
        "        \n",
        "        places = self.coor_discretizer_1.fit_transform(df['latitude'], df['longitude'])\n",
        "        places2 = self.coor_discretizer_2.fit_transform(df['latitude'], df['longitude'])\n",
        "        places4 = self.coor_discretizer_4.fit_transform(df['latitude'], df['longitude'])\n",
        "        places8 = self.coor_discretizer_8.fit_transform(df['latitude'], df['longitude'])\n",
        "        \n",
        "        \n",
        "        \n",
        "        texts = list(df['texts'].astype(str))\n",
        "        words = [word for list_words in texts for word in list_words.split()]\n",
        "        \n",
        "        self.vocab_dates = self.build_vocab(dates, dates_vocab_size, dates_vocab_mincount)\n",
        "        self.vocab_places = self.build_vocab(places, places_vocab_size, places_vocab_mincount)\n",
        "        \n",
        "        self.vocab_words = self.build_vocab(words, words_vocab_size, words_vocab_mincount)\n",
        "        \n",
        "        filtered_dates = set([i for i in range(len(dates)) if dates[i] in self.vocab_dates ])\n",
        "        filtered_places = set([i for i in range(len(places)) if places[i] in self.vocab_places])\n",
        "        \n",
        "        filtered_words = set([i for i in range(len(texts)) if any([word in self.vocab_words for word in texts[i].split()]) ])\n",
        "        filtered = list(filtered_dates.intersection(filtered_places).intersection(filtered_words))\n",
        "        \n",
        "        dates = [dates[i] for i in filtered]\n",
        "        days = [days[i] for i in filtered]\n",
        "        weeks = [weeks[i] for i in filtered]\n",
        "        months = [months[i] for i in filtered]\n",
        "\n",
        "        places = [places[i] for i in filtered]\n",
        "        places2 = [places2[i] for i in filtered]\n",
        "        places4 = [places4[i] for i in filtered]\n",
        "        places8 = [places8[i] for i in filtered]\n",
        "        \n",
        "        texts = [texts[i] for i in filtered]\n",
        "\n",
        "        self.word2idx, self.idx2word = buildIndexData(self.vocab_words)\n",
        "\n",
        "        self.date2idx, self.idx2date = buildIndexData(dates)\n",
        "        self.day2idx, self.idx2day = buildIndexData(days)\n",
        "        self.week2idx, self.idx2week = buildIndexData(weeks)\n",
        "        self.month2idx, self.idx2month = buildIndexData(months)\n",
        "\n",
        "        self.place2idx, self.idx2place = buildIndexData(places)\n",
        "        self.place2_2idx, self.idx2place2 = buildIndexData(places2)\n",
        "        self.place4_2idx, self.idx2place4 = buildIndexData(places4)\n",
        "        self.place8_2idx, self.idx2place8 = buildIndexData(places8)\n",
        "        \n",
        "        self.date2idx[\"<UNK>\"] = len(self.date2idx)\n",
        "        self.idx2date[len(self.date2idx)] = \"<UNK>\"\n",
        "        self.day2idx[\"<UNK>\"] = len(self.day2idx)\n",
        "        self.idx2day[len(self.day2idx)] = \"<UNK>\"\n",
        "        self.week2idx[\"<UNK>\"] = len(self.week2idx)\n",
        "        self.idx2week[len(self.week2idx)] = \"<UNK>\"\n",
        "        self.month2idx[\"<UNK>\"] = len(self.month2idx)\n",
        "        self.idx2month[len(self.month2idx)] = \"<UNK>\"\n",
        "\n",
        "        self.place2idx[\"<UNK>\"] = len(self.place2idx)\n",
        "        self.idx2place[len(self.place2idx)] = \"<UNK>\"\n",
        "        self.place2_2idx[\"<UNK>\"] = len(self.place2_2idx)\n",
        "        self.idx2place2[len(self.place2_2idx)] = \"<UNK>\"\n",
        "        self.place4_2idx[\"<UNK>\"] = len(self.place4_2idx)\n",
        "        self.idx2place4[len(self.place4_2idx)] = \"<UNK>\"\n",
        "        self.place8_2idx[\"<UNK>\"] = len(self.place8_2idx)\n",
        "        self.idx2place8[len(self.place8_2idx)] = \"<UNK>\"\n",
        "\n",
        "        idxsdates = [self.date2idx.get(date, self.date2idx[\"<UNK>\"]) for date in dates]\n",
        "        idxsdays = [self.day2idx.get(day, self.day2idx[\"<UNK>\"]) for day in days]\n",
        "        idxsweeks = [self.week2idx.get(week, self.week2idx[\"<UNK>\"]) for week in weeks]\n",
        "        idxsmonths = [self.month2idx.get(month, self.month2idx[\"<UNK>\"]) for month in months]\n",
        "        \n",
        "        idxsplaces = [self.place2idx.get(place, self.place2idx[\"<UNK>\"]) for place in places]\n",
        "        idxsplaces2 = [self.place2_2idx.get(place, self.place2_2idx[\"<UNK>\"]) for place in places2]\n",
        "        idxsplaces4 = [self.place4_2idx.get(place, self.place4_2idx[\"<UNK>\"]) for place in places4]\n",
        "        idxsplaces8 = [self.place8_2idx.get(place, self.place8_2idx[\"<UNK>\"]) for place in places8]\n",
        "\n",
        "        vocab_size = len(self.word2idx)\n",
        "        self.word2idx[\"<PAD>\"] = vocab_size\n",
        "        self.word2idx[\"<UNK>\"] = vocab_size+1 \n",
        "        self.word2idx[\"<START>\"] = vocab_size+2\n",
        "        self.word2idx[\"<END>\"] = vocab_size+3\n",
        "        \n",
        "\n",
        "        self.idx2word[vocab_size] = \"<PAD>\"\n",
        "        self.idx2word[vocab_size+1] = \"<UNK>\"\n",
        "        self.idx2word[vocab_size+2] = \"<START>\"\n",
        "        self.idx2word[vocab_size+3] = \"<END>\"\n",
        "\n",
        "        idxstexts = []\n",
        "        for text in texts:\n",
        "            indexed_text = [self.word2idx[\"<START>\"]] +[self.word2idx[word] for word in text.split() if word in self.vocab_words]+[self.word2idx[\"<END>\"]]\n",
        "            idxstexts.append(indexed_text)\n",
        "\n",
        "        \n",
        "        full_list = list(zip(idxsdates, idxsdays, idxsweeks, idxsmonths, idxsplaces, idxsplaces2, idxsplaces4, idxsplaces8, idxstexts))\n",
        "        clean_list = [[x[0]]+ [x[1]]+ [x[2]]+ [x[3]]+[x[4]]+ [x[5]]+[x[6]]+ [x[7]]+ x[8] for x in full_list]\n",
        "        return clean_list\n",
        "\n",
        "    def transform(self, df):\n",
        "        dates  = self.time_discretizer_hour.fit_transform(df['created_at'])\n",
        "        days   = self.time_discretizer_day.fit_transform(df['created_at'])\n",
        "        weeks  = self.time_discretizer_week.fit_transform(df['created_at'])\n",
        "        months = self.time_discretizer_month.fit_transform(df['created_at'])\n",
        "        \n",
        "        places  = self.coor_discretizer_1.fit_transform(df['latitude'], df['longitude'])\n",
        "        places2 = self.coor_discretizer_2.fit_transform(df['latitude'], df['longitude'])\n",
        "        places4 = self.coor_discretizer_4.fit_transform(df['latitude'], df['longitude'])\n",
        "        places8 = self.coor_discretizer_8.fit_transform(df['latitude'], df['longitude'])\n",
        "\n",
        "        idxsdates  = [self.date2idx.get(date, self.date2idx[\"<UNK>\"]) for date in dates]\n",
        "        idxsdays   = [self.day2idx.get(day, self.day2idx[\"<UNK>\"]) for day in days]\n",
        "        idxsweeks  = [self.week2idx.get(week, self.week2idx[\"<UNK>\"]) for week in weeks]\n",
        "        idxsmonths = [self.month2idx.get(month, self.month2idx[\"<UNK>\"]) for month in months]\n",
        "        \n",
        "        idxsplaces  = [self.place2idx.get(place, self.place2idx[\"<UNK>\"]) for place in places]\n",
        "        idxsplaces2 = [self.place2_2idx.get(place, self.place2_2idx[\"<UNK>\"]) for place in places2]\n",
        "        idxsplaces4 = [self.place4_2idx.get(place, self.place4_2idx[\"<UNK>\"]) for place in places4]\n",
        "        idxsplaces8 = [self.place8_2idx.get(place, self.place8_2idx[\"<UNK>\"]) for place in places8]\n",
        "        \n",
        "        idxstexts = []\n",
        "        for text in df['texts'].astype(str):\n",
        "            indexed_text = [self.word2idx[\"<START>\"]] +[self.word2idx[word] for word in text.split() if word in self.vocab_words]+[self.word2idx[\"<END>\"]]\n",
        "            idxstexts.append(indexed_text)\n",
        "\n",
        "        full_list = list(zip(idxsdates, idxsdays, idxsweeks, idxsmonths, idxsplaces, idxsplaces2, idxsplaces4, idxsplaces8, idxstexts))\n",
        "        clean_list = [[x[0]]+ [x[1]]+ [x[2]]+ [x[3]]+[x[4]]+ [x[5]]+[x[6]]+ [x[7]]+ x[8] for x in full_list if ((x[0] != None) and (x[4] != None) and (len(x[8]) > 1)) ]\n",
        "        return clean_list\n",
        "      \n",
        "    def Item2idx(self, item):\n",
        "        return self.item2idx.get(item, -1)\n",
        "\n",
        "    def Idx2item(self, index):\n",
        "        return self.idx2item.get(index, None)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5bnBrnPWP8H",
        "colab_type": "text"
      },
      "source": [
        "##Params pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvIbRRhlc5xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_data = 'drive/My Drive/Colab Notebooks/STTD/Data/'\n",
        "dataset = \"tweetsNY.csv\"\n",
        "\n",
        "places_vocab_size = 1000\n",
        "            \n",
        "dates_vocab_mincount= 0 \n",
        "places_vocab_mincount=0\n",
        "words_vocab_mincount=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC0oUcrLdv86",
        "colab_type": "text"
      },
      "source": [
        "###Split Train-Test and Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHVzbqxng-e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(path_data+dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGA4ywk-WLmy",
        "colab_type": "code",
        "outputId": "532c5e88-f60f-4101-d40f-afdb770ce51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "columns = list(df.columns.values)\n",
        "print('Done load data')\n",
        "\n",
        "if not ('created_at' in columns):\n",
        "    print(\"Need a column name created_at with timestamps\")\n",
        "                    \n",
        "if not ('latitude' in columns and 'longitude' in columns):\n",
        "    print(\"Need a columns names latitude and longitude\")\n",
        "                    \n",
        "if not ('texts' in columns):\n",
        "\n",
        "    print(\"Need a column name texts with texts\")\n",
        "                    \n",
        "length = len(df)                   \n",
        "                    \n",
        "train_range =  np.r_[0:int(0.6*length)]\n",
        "val_range =  np.r_[int(0.6*length):int(0.8*length)]\n",
        "test_range =  np.r_[int(0.8*length):length]\n",
        "                    \n",
        "train = df.loc[train_range, :]\n",
        "val = df.loc[val_range, :]\n",
        "test = df.loc[test_range, :]\n",
        "\n",
        "indexer = Indexer()\n",
        "print('Start indexing ')\n",
        "            \n",
        "train_data = indexer.fit_transform(train, \n",
        "                                   dates_vocab_mincount=dates_vocab_mincount, \n",
        "                                   words_vocab_mincount=words_vocab_mincount, \n",
        "                                   places_vocab_mincount=places_vocab_mincount,\n",
        "                                   places_vocab_size=places_vocab_size)\n",
        "print('Done indexing train')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done load data\n",
            "Start indexing \n",
            "Done indexing train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mck6PYty5-L1",
        "colab_type": "code",
        "outputId": "79d6c19f-8034-4b1f-a9c2-8704e882614b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "val_data  = indexer.transform(val)\n",
        "print('Done indexing val')\n",
        "test_data = indexer.transform(test)\n",
        "print('Done indexing test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done indexing val\n",
            "Done indexing test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsntO7HU_Anq",
        "colab_type": "code",
        "outputId": "e2c50c19-7ba2-472a-cf55-5066a479e890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158567\n",
            "95859\n",
            "95860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_bGD8ks_PfW",
        "colab_type": "code",
        "outputId": "42db91fc-9049-44e9-b2b6-d29d3f4de873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(train_data[0])\n",
        "print(val_data[0])\n",
        "print(test_data[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 3, 3, 3, 27, 28, 24, 16, 1552, 91, 229, 966, 1553]\n",
            "[12, 0, 0, 7, 147, 123, 92, 62, 1552, 1500, 500, 51, 1324, 1463, 1411, 1553]\n",
            "[10, 1, 2, 7, 1000, 569, 290, 156, 1552, 1553]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6m5UOXClc5v",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRUHHFCyKAEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_len = 8\n",
        "sent_len = 10\n",
        "maxlen = context_len + sent_len\n",
        "train_data = [exam[:maxlen-1] for exam in train_data]\n",
        "val_data = [exam[:maxlen-1] for exam in val_data]\n",
        "test_data = [exam[:maxlen-1] for exam in test_data]\n",
        "\n",
        "\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=indexer.word2idx[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "val_data =  keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=indexer.word2idx[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=maxlen)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=indexer.word2idx[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l0y83gtWDOT",
        "colab_type": "code",
        "outputId": "db6abd8a-c412-40ee-d9b3-c0add7f4fa8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#train_data = [[tup[0],tup[1],word] for tup in train_data for word in tup[2:] ]\n",
        "#val_data = [ [tup[0],tup[1],word] for tup in val_data for word in tup[2:] ]\n",
        "print(len(train_data), len(val_data), len(test_data))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158567 95860 95860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXRU83-Mn9nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 1000\n",
        "fake = 10\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(lambda: train_data, tf.int32, output_shapes=[None])\n",
        "dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(lambda: val_data, tf.int32, output_shapes=[None])\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PppHCtB8SqZe",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcHjPIZXYzSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = len(train_data)//BATCH_SIZE\n",
        "epochs = 3\n",
        "embedding_dim = 128\n",
        "enc_units = 256\n",
        "vocab_time_size = len(indexer.idx2date)\n",
        "vocab_day_size = len(indexer.idx2day)\n",
        "vocab_week_size = len(indexer.idx2week)\n",
        "vocab_month_size = len(indexer.idx2month)\n",
        "\n",
        "vocab_place_size = len(indexer.idx2place)\n",
        "vocab_place2_size = len(indexer.idx2place2)\n",
        "vocab_place4_size = len(indexer.idx2place4)\n",
        "vocab_place8_size = len(indexer.idx2place8)\n",
        "\n",
        "vocab_word_size = len(indexer.idx2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqjN2uU2fm9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_STTD(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(LM_STTD, self).__init__()\n",
        "        self.embed_time = tf.keras.layers.Embedding(vocab_time_size, embedding_dim)\n",
        "        self.embed_day = tf.keras.layers.Embedding(vocab_day_size, embedding_dim)\n",
        "        self.embed_week = tf.keras.layers.Embedding(vocab_week_size, embedding_dim)\n",
        "        self.embed_month = tf.keras.layers.Embedding(vocab_month_size, embedding_dim)\n",
        "\n",
        "        self.embed_loc = tf.keras.layers.Embedding(vocab_place_size, embedding_dim)\n",
        "        self.embed_loc2 = tf.keras.layers.Embedding(vocab_place2_size, embedding_dim)\n",
        "        self.embed_loc4 = tf.keras.layers.Embedding(vocab_place4_size, embedding_dim)\n",
        "        self.embed_loc8 = tf.keras.layers.Embedding(vocab_place8_size, embedding_dim)\n",
        "\n",
        "        self.embed_word = tf.keras.layers.Embedding(vocab_word_size, embedding_dim)\n",
        "        \n",
        "        self.enc_units = enc_units\n",
        "        \n",
        "        self.fc_enc_emb_times = tf.keras.layers.Dense(embedding_dim, use_bias=True)\n",
        "        self.fc_enc_emb_locs = tf.keras.layers.Dense(embedding_dim, use_bias=True)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        #PREDICT\n",
        "        self.fc_word = tf.keras.layers.Dense(vocab_word_size, use_bias=True)\n",
        "        \n",
        "        self.rnn_pre = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, batch, context_flag= \"tp\"):\n",
        "        times_batch = tf.squeeze(batch[:,:1])\n",
        "        days_batch = tf.squeeze(batch[:,1:2])\n",
        "        weeks_batch = tf.squeeze(batch[:,2:3])\n",
        "        months_batch = tf.squeeze(batch[:,3:4])\n",
        "\n",
        "        locs_batch = tf.squeeze(batch[:,4:5])\n",
        "        locs2_batch = tf.squeeze(batch[:,5:6])\n",
        "        locs4_batch = tf.squeeze(batch[:,6:7])\n",
        "        locs8_batch = tf.squeeze(batch[:,7:8])\n",
        "\n",
        "        texts_batch = tf.squeeze(batch[:,8:-1])\n",
        "\n",
        "        texts_embedded = self.embed_word(texts_batch)\n",
        "        if context_flag == \"\":\n",
        "            output, state = self.rnn_pre(texts_embedded)\n",
        "            return self.fc_word(output)\n",
        "\n",
        "        contexts = []\n",
        "        contexts_expanded = []\n",
        "\n",
        "        if \"t\" in context_flag:\n",
        "            times_embedded = tf.expand_dims(self.embed_time(times_batch), 1)\n",
        "            days_embedded = tf.expand_dims(self.embed_day(days_batch), 1)\n",
        "            weeks_embedded = tf.expand_dims(self.embed_week(weeks_batch), 1)\n",
        "            months_embedded = tf.expand_dims(self.embed_month(months_batch), 1)\n",
        "            t = tf.concat([times_embedded, days_embedded, weeks_embedded, months_embedded], 2)\n",
        "            times_context = self.relu(self.fc_enc_emb_times(t))\n",
        "            contexts.append(times_context)\n",
        "        \n",
        "        if \"p\" in context_flag:\n",
        "            locs_embedded = tf.expand_dims(self.embed_loc(locs_batch), 1)\n",
        "            places2_embedded = tf.expand_dims(self.embed_loc2(locs2_batch), 1)\n",
        "            places4_embedded = tf.expand_dims(self.embed_loc4(locs4_batch), 1)\n",
        "            places8_embedded = tf.expand_dims(self.embed_loc8(locs8_batch), 1)\n",
        "            p = tf.concat([locs_embedded, places2_embedded, places4_embedded, places8_embedded], 2)\n",
        "            locs_context = self.relu(self.fc_enc_emb_locs(p))\n",
        "            contexts.append(locs_context)\n",
        "        \n",
        "        context_tensor = tf.concat(contexts, 2)\n",
        "        output, state = self.rnn_pre(texts_embedded, initial_state=tf.squeeze(context_tensor))\n",
        "\n",
        "        context_expanded = tf.broadcast_to(context_tensor, [context_tensor.shape[0],output.shape[1],context_tensor.shape[2]])\n",
        "        \n",
        "        \n",
        "        return self.fc_word(tf.concat(context_expanded, 2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "# Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(batch):\n",
        "    loss = 0\n",
        "    loss_time = 0\n",
        "    loss_place = 0\n",
        "    loss_text = 0\n",
        "    \n",
        "    words_batch = tf.squeeze(batch[:,9:])\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions_word = model(batch)\n",
        "        loss_word = loss_function(words_batch, predictions_word)\n",
        "        loss = loss_word\n",
        "    \n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    \n",
        "    train_acc_text(words_batch, predictions_word)\n",
        "    train_loss_text(loss_word)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def val_step(batch):\n",
        "    words_batch = tf.squeeze(batch[:,9:])\n",
        "\n",
        "    predictions_word = model(batch)\n",
        "    val_acc_text(words_batch, predictions_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVlE4dVxo9BQ",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    loss = tf.reduce_mean(loss_)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3CEhg4UZ39E",
        "colab_type": "text"
      },
      "source": [
        "##Trackers for Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzB7K0ItTqyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "\n",
        "train_loss_time = tf.keras.metrics.Mean('train_loss_time', dtype=tf.float32)\n",
        "train_loss_place = tf.keras.metrics.Mean('train_loss_place', dtype=tf.float32)\n",
        "train_loss_text = tf.keras.metrics.Mean('train_loss_text', dtype=tf.float32)\n",
        "\n",
        "train_acc_time = tf.keras.metrics.SparseCategoricalAccuracy('train_acc_time')\n",
        "train_acc_place = tf.keras.metrics.SparseCategoricalAccuracy('train_acc_place')\n",
        "train_acc_text = tf.keras.metrics.SparseCategoricalAccuracy('train_acc_text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIkHvAK0um-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
        "\n",
        "val_loss_time = tf.keras.metrics.Mean('val_loss_time', dtype=tf.float32)\n",
        "val_loss_place = tf.keras.metrics.Mean('val_loss_place', dtype=tf.float32)\n",
        "val_loss_text = tf.keras.metrics.Mean('val_loss_text', dtype=tf.float32)\n",
        "\n",
        "val_acc_time = tf.keras.metrics.SparseCategoricalAccuracy('val_acc_time')\n",
        "val_acc_place = tf.keras.metrics.SparseCategoricalAccuracy('val_acc_place')\n",
        "val_acc_text = tf.keras.metrics.SparseCategoricalAccuracy('val_acc_text')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzQdUc7v7FTV",
        "colab_type": "text"
      },
      "source": [
        "##Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQgRTnC2A9As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_'+str(epochs)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)                           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQXTx77BCfjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_dir = 'logs/gradient_tape_epochs_'+str(epochs)+'/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "val_log_dir = 'logs/gradient_tape_epochs_'+str(epochs)+'/' + current_time + '/val'\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPm7xNKeb0WQ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-f1CrepPJ8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model =  LM_STTD(vocab_time_size, vocab_space_size, vocab_word_size, embedding_dim, enc_units)\n",
        "model =  LM_STTD()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRSPg1CKA89S",
        "colab_type": "code",
        "outputId": "efcb02c4-d691-4e23-8f95-061751176676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "EPOCHS = epochs\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, inp) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        \n",
        "        #batch_loss = train_step(inp,representtext_hidden)\n",
        "        batch_loss = train_step(inp)\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "\n",
        "        #if batch % 1000 == 0:\n",
        "        #    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "        #                                             batch,\n",
        "        #                                             batch_loss.numpy()))\n",
        "    \n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "        tf.summary.scalar('loss_time', train_loss_time.result(), step=epoch)\n",
        "        tf.summary.scalar('loss_place', train_loss_place.result(), step=epoch)\n",
        "        tf.summary.scalar('loss_text', train_loss_text.result(), step=epoch)\n",
        "    \n",
        "        tf.summary.scalar('acc_time', train_acc_time.result(), step=epoch)\n",
        "        tf.summary.scalar('acc_place', train_acc_place.result(), step=epoch)\n",
        "        tf.summary.scalar('acc_text', train_acc_text.result(), step=epoch)\n",
        "        \n",
        "    for (batch, inp) in enumerate(val_dataset):\n",
        "        val_step(inp)    \n",
        "    \n",
        "    with val_summary_writer.as_default():  \n",
        "        tf.summary.scalar('val_acc_time', val_acc_time.result(), step=epoch)  \n",
        "        tf.summary.scalar('val_acc_place', val_acc_place.result(), step=epoch)\n",
        "        tf.summary.scalar('val_acc_text', val_acc_text.result(), step=epoch)\n",
        "        \n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "    # Reset metrics every epoch\n",
        "    train_loss.reset_states()\n",
        "    train_loss_time.reset_states()    \n",
        "    train_loss_place.reset_states()    \n",
        "    train_loss_text.reset_states()  \n",
        "    \n",
        "    train_acc_time.reset_states()    \n",
        "    train_acc_place.reset_states() \n",
        "    train_acc_text.reset_states()\n",
        "    \n",
        "    val_acc_time.reset_states()    \n",
        "    val_acc_place.reset_states()\n",
        "    val_acc_text.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['lm_sttd/embedding_8/embeddings:0', 'lm_sttd/gru/kernel:0', 'lm_sttd/gru/recurrent_kernel:0', 'lm_sttd/gru/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['lm_sttd/embedding_8/embeddings:0', 'lm_sttd/gru/kernel:0', 'lm_sttd/gru/recurrent_kernel:0', 'lm_sttd/gru/bias:0'] when minimizing the loss.\n",
            "Epoch 1 Loss 3.8199\n",
            "Time taken for 1 epoch 32.394216775894165 sec\n",
            "\n",
            "Epoch 2 Loss 2.7360\n",
            "Time taken for 1 epoch 27.706735849380493 sec\n",
            "\n",
            "Epoch 3 Loss 2.4542\n",
            "Time taken for 1 epoch 27.740156650543213 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rNbQ88-RWfN",
        "colab_type": "code",
        "outputId": "3cef85b8-0e0d-44df-f370-163d3fc8d383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape_epochs_60"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1333), started 3:44:47 ago. (Use '!kill 1333' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzdJkGLS8fX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights(checkpoint_prefix.format(epoch=0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}